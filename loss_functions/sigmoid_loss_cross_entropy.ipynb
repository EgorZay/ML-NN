{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid function and cross-entropy loss\n",
    "\n",
    "*Imagine you are to deal with a classification problem, whether the output should be in $N_+$. You might thus want to ask what the probability of your model to classify the output is. In that case, you might well apply sigmoid function to discover this aspect.*\n",
    "\n",
    "*Moreover, to evaluate a goodness-of-fit of your model you may use cross-entropy loss. As a rule, it well fits with sigmoid function.*\n",
    "\n",
    "So, let's break down right into it.\n",
    "\n",
    "\n",
    "\n",
    "## Sigmoid function _\n",
    "\n",
    "Sigmoid function is just a special case of logistic function (or not-smoothed step function) to return a probability. At least, that is what I am aware of... It is defined as following:\n",
    "\n",
    "\\begin{align}\n",
    "\\gamma(Y) = \\frac{1}{1+\\exp^{-Y}},\n",
    "\\end{align}\n",
    "*where $Y$ is the predicted output of your model.*\n",
    "\n",
    "\n",
    "It is easy to implement this with codes using hands or libraries. Let's look at TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "beta = tf.Variable([1.])\n",
    "alpha = tf.Variable([5.])\n",
    "x = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here we define **beta** and **alpha** as variables that are already written in the memory as **1** and **5**, accordingly. Make sure to keep them as floats. $x$ is a placeholder which means that it is not yet assigned. We will pass digits to $x$ later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_true = tf.placeholder()\n",
    "Y = tf.matmul(x, beta) + alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that **y_true** is the actual labels of data, and **Y** is our model - basic linear regression with $\\beta = 1$ and $\\alpha = 5$. It overall looks like following:\n",
    "\n",
    "\\begin{align}\n",
    "Y = \\alpha + \\beta x \\\\\n",
    "Y = 5 + x\n",
    "\\end{align}\n",
    "\n",
    "To apply sigmoid we can use in-built command or write it manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sigmoid = tf.nn.sigmoid(Y)\n",
    "# sigmoid = tf.nn.sigmoid(tf.matmul(x, beta) + alpha)\n",
    "\n",
    "def sigmoidFunction(Y):\n",
    "    return 1. / (1. + np.exp(-Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Cross-entropy loss\n",
    "\n",
    "So, to evaluate our goodness-of-fit, a.k.a. how good our model is, we need to consider a loss function. Basically, if you use sigmoid you infer with cross-entropy. The most indepth presentaion of cross-entropy validation is [here](http://colah.github.io/posts/2015-09-Visual-Information/).\n",
    "\n",
    "After you've read it, you may now proceed to codes. However, TensorFlow has special method to evaluate cross-entropy and make it numerically stable. It automatically applies sigmoid first and then calculate the cost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true, logits=y)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Or you may always use the codes below to calculate the loss manually. The actual formulae of cross-entropy loss is the following:\n",
    "\n",
    "\\begin{align}\n",
    "H_p(Y_t) = \\frac{1}{n}\\sum_n^m{Y_t log_2(\\frac{1}{\\gamma(Y)})},\n",
    "\\end{align}\n",
    "where $Y_t$ is **y_true** (or our true labels), $Y$ is the vector (or any other n-dimensional thing) of model predictions and $\\gamma(Y)$ is sigmoid transformed $Y$. As long as you work with multi-dimensional data (and it happens like most of the times) you should get a mean cross-entropy loss as it is a good heuristic to understand a goodness-of-fit.\n",
    "\n",
    "Actually, this is equal to:\n",
    "\n",
    "\\begin{align}\n",
    "H_p(Y_t) = -\\frac{1}{n}^m\\sum_n{Y_t log_2(\\gamma(Y)}.\n",
    "\\end{align}\n",
    "\n",
    "And due to basic logarithmic property we obtain: \n",
    "\\begin{align}\n",
    "log(\\frac{1}{\\alpha}) = -log(\\alpha).\n",
    "\\end{align}\n",
    "\n",
    "We may manually calculate that via Python as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crossEntropy(y_true, pred=y):\n",
    "    return np.mean(-np.sum(y_true * np.log(sigmoidFunction(pred))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing to do is to give it a go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ses = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "ses.run(init)\n",
    "print(sess.run(Y, {x: [1, 2, 3, 4]}))\n",
    "\n",
    "print(sess.run(cross_entropy, {x: [1, 2, 3, 4], y_true: [-5.5, -6.5, 7.5, 8.5]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize variables ($\\beta$ and $\\alpha$) with **init** in the session. Then, we build a line with $x = (x_1, x_2, x_3, x_4)$ | $x = (1, 2, 3, 4)$. In this case, you may think of $x$ and $Y$ as features and predictions. Last **print** is to evaluate cross-entropy loss of current model with $x$ as given features and real labels $Y_t$ = **y_true**.\n",
    "\n",
    "Thus, by obtaining mean cross-entropy loss we may say if our model is good or not. To make it better, we need to tweak $\beta$ and $\alpha$, implying we do not want to change the model at all. That is the next topic. Thanks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
