{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Activation functions\n",
    "### Sigmoid function, *a.k.a.* $\\sigma(z)$\n",
    "___\n",
    "\n",
    "In general, no model can live without activation functions unless we elaborate on simple binary decision tasks. In this specific case we may introduce [Perceptron](https://en.wikipedia.org/wiki/Perceptron) to save time of computation. Anyway, it is rarely used nowadays and only serves as an introductory part to neural networks.\n",
    "\n",
    "People tend to elaborate on more complicated problems that require much more flexibility than Perceptron has. While they build complex models containing numerous weights and biases they want to have complete influence on the workflow. \n",
    "\n",
    "Thus, they assume that minor changes of parameters, i.e. weights and biases, should lead to minor alterations of model outputs. Obviously, Perceptron cannot be used in that scenario as it encapsulates just two commands to activate a neuron: 'YES' or 'NO', 1 or 0. So one can pass to Perceptron only a binary variable: $x = (x_1, x_2, \\dots, x_n), x \\in \\{0; 1\\}$\n",
    "\n",
    "There is nothing hidden between those two digits, so it is a rather simple mechanism mathematically presented as:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "  output = \\left\\{ \n",
    "    \\begin{array}{ll} \n",
    "      1 & if  w\\cdot x + b > 0 \\\\\n",
    "      0 & if  w\\cdot x + b \\leq 0\n",
    "    \\end{array}\n",
    "  \\right.\n",
    "\\end{eqnarray}\n",
    "\n",
    "As seen from the system above, the output variable may be written as a linear model: $output(x) = w\\cdot x + b$. It is just a general suggestion because it is easy to work with. Perceptron may still be worth studying if you feel like computing the elementary logical functions such as $AND, OR, NAND$. More complex tasks require somewhat different approach.\n",
    "\n",
    "Remember that we require a gradual change of the output To achieve that we use sigmoids! It is a sort of a modified perceptrons, in a way that the small changes of weights and biases ($w, b$) cause only small changes of the output. It is a main factor how a network of sigmoid neurons may learn. Moreover, we may pass a completely different input vector: $x = (x_1, \\dots, x_n), x \\in [0, 1]$, which is great! The output, of course, is not 0 or 1 anymore. Instead, it equals to $\\sigma(w\\cdot x + b)$, where $\\sigma$ is the sigmoid function, and is defined by:\n",
    "\n",
    "\\begin{align}\n",
    "\\sigma(z) \\equiv \\frac{1}{1+\\exp^{-z}},\n",
    "\\end{align}\n",
    "\n",
    "Yeah, in our case $z$ is the output. If you rewrite the equation above explicitly, you will get:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{1}{1+\\exp(-\\sum_n{w_nx_n-b})}\n",
    "\\end{align}\n",
    "\n",
    "It is not hard to realise if you know that for each input $x_1, x_2, \\dots$ you have its weight $w_1, w_2, \\dots$. More inputs you have, more weights a network has to tinker with, and thus broader a summation formulae will be.\n",
    "\n",
    "This approach is neat and it operates really similar to Perceptron. Suppose that $z \\equiv w\\cdot x + b$ is a large positive number. Then $e^{-z} \\approx 0$ and so $\\sigma(z) \\approx 1$. The sigmoid neuron fires just like a perceptron neuron would. On the other hand, suppose that $z \\equiv w\\cdot x + b$ is very negative. Then $e^{-z} \\rightarrow \\infty$ and thus $\\sigma(z) \\approx 0$ which is another Perceptron scenario.\n",
    "\n",
    "So what is that all about? Why utilize sigmoid neurons in a network? Well, as I have already mentioned, we want gradual changes of the output to let a network learn. This crucial fact will allow us to use special techniques to determine the best weights and biases for the network. Such graduality is fine provided we look for a flexible model.\n",
    "\n",
    "If we use sigmoid neurons the change of the output is then defined by:\n",
    "\n",
    "\\begin{eqnarray} \n",
    "  \\Delta output \\approx \\sum_n \\frac{\\partial \\, output}{\\partial w_n}\n",
    "  \\Delta w_n + \\frac{\\partial \\, output}{\\partial b} \\Delta b,\n",
    "\\end{eqnarray}\n",
    "\n",
    "where we see some partial derivatives and a summation according to the amount of inputs. And yes, you might have already guessed, the output $z$ receives a scope of numbers such as $z \\in [0; 1]$ and indicates the 'degree of confidence' that the input relates to a desired value.\n",
    "\n",
    "The theory might look cumbersome but in fact it is not. In python it takes no effort to calculate it. I will hereby show you how to define a sigmoid function mathematically and with TensorFlow library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "w = tf.Variable([0.], tf.float32)\n",
    "b = tf.Variable([0.], tf.float32)\n",
    "x = tf.placeholder(tf.float32)\n",
    "z = tf.matmul(w, x) + b\n",
    "\n",
    "z_sigmoid = tf.nn.sigmoid(z)\n",
    "# sigmoid = tf.nn.sigmoid(tf.matmul(w, x) + b)\n",
    "\n",
    "def sigmoidFunction(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Basically, this is the simplest one you will ever see. We pass to the output 1-dimensional weight vector and 1-dimensional bias vector. Of course, you will surely need to extend it when you stumble on a real problem to build a network. $w, b$ are explicitly defined and written in RAM while $x$ is a placeholder and waits to be provided with a number. Anyway, it is another topic, and you might want to take a look at the [official TensorFlow website](https://www.tensorflow.org).\n",
    "\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
